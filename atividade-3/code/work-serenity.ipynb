{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitoramento de presença e bem-estar em ambiente de trabalho\n",
    "\n",
    "Em ambientes de trabalho, a detecção de emoções pode se útil para identificar situações de estresse ou insatisfação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import cv2\n",
    "from deepface import DeepFace\n",
    "import face_recognition\n",
    "from io import BytesIO\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url de acesso da emotions-api\n",
    "base_url = 'http://localhost:3333'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '8503b9d3-ce33-4a03-9d62-2e95dab4588d',\n",
       "  'name': 'Laciene Melo Garcia',\n",
       "  'avatar_url': 'http://localhost:3333/uploads/4cbaff2534d2-foto-perfil-laciene.jpeg'},\n",
       " {'id': '3c831627-d1ae-46ae-99d6-8bc1ca85e000',\n",
       "  'name': 'Lourival Oliveira Melo',\n",
       "  'avatar_url': 'http://localhost:3333/uploads/aa2803df0511-perfil-papai.jpeg'},\n",
       " {'id': 'e42ca150-ecc2-4102-b923-917ed7b1b60c',\n",
       "  'name': 'Leonaro Nunes Conçalves',\n",
       "  'avatar_url': 'http://localhost:3333/uploads/047b69e79667-perfil-leo.jpeg'},\n",
       " {'id': 'eef2855f-dfa0-4c28-a6d7-fa1ba4257d43',\n",
       "  'name': 'Igor Melo Garcia',\n",
       "  'avatar_url': 'http://localhost:3333/uploads/fcf67e515a71-perfil-igor.jpeg'},\n",
       " {'id': '364390ee-d9fb-43ef-bbfd-6a91649beff1',\n",
       "  'name': 'Miguel Ângelo Mocbel',\n",
       "  'avatar_url': 'http://localhost:3333/uploads/2e21ccb770eb-perfil-miguel.jpeg'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Realiza uma solicitação GET para obter a lista de usuários\n",
    "response = requests.get(f'{base_url}/user/list')\n",
    "users = response.json()\n",
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa uma lista de objetos contendo codificações faciais e nomes correspondentes\n",
    "known_faces = []\n",
    "\n",
    "# Itera sobre a lista de usuários\n",
    "\n",
    "for user in users:\n",
    "    # carrega a imagem do usuário\n",
    "    user_image_url = user['avatar_url']\n",
    "\n",
    "    # baixa a imagem usando requests\n",
    "    response_image = requests.get(user_image_url)\n",
    "    image_data = BytesIO(response_image.content)\n",
    "\n",
    "    # converte a imagem para um formato suportado pelo face-recognition\n",
    "    pil_image = Image.open(image_data).convert(\"RGB\")\n",
    "    user_image = np.array(pil_image)\n",
    "\n",
    "    # realiza o processo de reconhecimento facial\n",
    "    user_face_encoding = face_recognition.face_encodings(user_image)[0]\n",
    "\n",
    "    # adiciona a codificação facial o nome à lista como um objeto\n",
    "    known_faces.append({'id': user['id'], 'encoding': user_face_encoding, 'name': user['name']})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializando a webcam\n",
    "video_capture = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00,  6.72it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 27.59it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 24.03it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 19.49it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 11.51it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.24it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 32.61it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 31.91it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 23.79it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 27.15it/s]\n",
      "Action: emotion: 100%|██████████| 1/1 [00:00<00:00, 24.04it/s]\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Capturando um frame do vídeo\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # encontrando todas as faces no frame\n",
    "    face_locations = face_recognition.face_locations(frame)\n",
    "    face_encodings = face_recognition.face_encodings(frame, face_locations)\n",
    "\n",
    "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "        # verificando se a face é conhecida\n",
    "        matches = face_recognition.compare_faces([user['encoding'] for user in known_faces], face_encoding)\n",
    "\n",
    "        name = \"Desconhecido\"\n",
    "\n",
    "        # se uma correspondência for encontrada, use o nome correspondente\n",
    "        if True in matches:\n",
    "            first_match_index = matches.index(True)\n",
    "            name = known_faces[first_match_index]['name']\n",
    "            user_id = known_faces[first_match_index]['id']\n",
    "\n",
    "            # Adicionado para o reconhecimento de emoções\n",
    "            roi = frame[max(0, top):min(frame.shape[0], bottom), max(0, left):min(frame.shape[1], right)]\n",
    "\n",
    "            result = DeepFace.analyze(roi, actions='emotion', enforce_detection=False)\n",
    "            \n",
    "            # 'result' é uma lista, então pegamos o primeiro elemento [0]\n",
    "            first_result = result[0]\n",
    "\n",
    "            # Agora podemos acessar 'dominant_emotion' no primeiro resultado\n",
    "            emotion = first_result['dominant_emotion']\n",
    "            message = \"Registrada\"\n",
    "\n",
    "            # cria o registro da presença e emoção\n",
    "            createEmotion = requests.post(f'{base_url}/emotion/create', json={\n",
    "                \"mood\": emotion,\n",
    "                \"user_id\": user_id\n",
    "            })\n",
    "\n",
    "        # Desenhando um retângulo ao redor do rosto\n",
    "        cv2.rectangle(frame, pt1=(left, top), pt2=(right, bottom), color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        # exibindo o nome da pessoa identificada\n",
    "        font = cv2.FONT_HERSHEY_DUPLEX\n",
    "        if user_id :\n",
    "            cv2.putText(frame, f\"id: {user_id}\", (left + 6, bottom + 20), font, 0.5, (255, 255, 255), 1)\n",
    "            cv2.putText(frame, f\"Presence: {message}\", (left + 6, bottom + 80), font, 0.5, (255, 255, 255), 1)\n",
    "            \n",
    "        cv2.putText(frame, f\"User: {name}\", (left + 6, bottom + 40), font, 0.5, (255, 255, 255), 1)\n",
    "        cv2.putText(frame, f\"Emotion: {emotion}\", (left + 6, bottom + 60), font, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "    # exibindo o frame resultante\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # saindo do loop quando a tecla 'q' é pressionada\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Liberando os recursos\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual-environment",
   "language": "python",
   "name": "virtual-environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
